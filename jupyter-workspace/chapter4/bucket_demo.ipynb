{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc11aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/04 02:24:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL 저장하기\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.jars\", \"/jars/redshift-jdbc42-2.1.0.19.jar\")\\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8de86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift와 연결해서 DataFrame으로 로딩하기\n",
    "url = \"jdbc:redshift://learnde.cduaw970ssvt.ap-northeast-2.redshift.amazonaws.com:5439/dev?user=guest&password=Guest1234\"\n",
    "\n",
    "df_user_session_channel = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"raw_data.user_session_channel\") \\\n",
    "    .load()\n",
    "\n",
    "df_session_timestamp = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"raw_data.session_timestamp\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32204927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+--------------------+--------------------+\n",
      "|userid|           sessionid| channel|           sessionid|                  ts|\n",
      "+------+--------------------+--------+--------------------+--------------------+\n",
      "|  1501|0135456d6a3c1051f...|  Google|0135456d6a3c1051f...|2019-09-24 14:49:...|\n",
      "|   876|01a416a7e28d0d229...|Facebook|01a416a7e28d0d229...|2019-05-26 14:23:...|\n",
      "|   243|0226aa5193c66d990...|  Google|0226aa5193c66d990...|2019-07-01 23:04:...|\n",
      "|  2776|029bf49b584c641f0...|Facebook|029bf49b584c641f0...|2019-11-11 20:37:...|\n",
      "|   939|02b8d6c2775b756de...|  Google|02b8d6c2775b756de...|2019-09-01 15:29:...|\n",
      "|  2133|02ea66ee24d57e285...| Organic|02ea66ee24d57e285...| 2019-10-28 22:50:40|\n",
      "|   771|02f6364bf207d6837...|   Naver|02f6364bf207d6837...|2019-09-07 18:24:...|\n",
      "|  1961|0302915889fa38fe5...| Youtube|0302915889fa38fe5...| 2019-11-29 15:16:49|\n",
      "|  2139|039cc47f960921e3c...|   Naver|039cc47f960921e3c...| 2019-08-01 18:19:34|\n",
      "|   599|03ce0331cbd983f16...| Organic|03ce0331cbd983f16...|2019-07-18 13:49:...|\n",
      "+------+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "join_expr = df_user_session_channel.sessionid == df_session_timestamp.sessionid\n",
    "df_join = df_user_session_channel.join(df_session_timestamp, join_expr, \"inner\")\n",
    "df_join.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14154ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/04 02:20:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/04 02:20:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/04 02:20:04 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/04 02:20:04 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.21.0.3\n",
      "23/10/04 02:20:05 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/04 02:20:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/10/04 02:20:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/10/04 02:20:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/04 02:20:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS bk_usc\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS bk_st\")\n",
    "\n",
    "df_user_session_channel.write.mode(\"overwrite\").bucketBy(3, \"sessionid\").saveAsTable(\"bk_usc\")\n",
    "df_session_timestamp.write.mode(\"overwrite\").bucketBy(3, \"sessionid\").saveAsTable(\"bk_st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c820a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------+--------------------+--------------------+\n",
      "|userid|           sessionid|  channel|           sessionid|                  ts|\n",
      "+------+--------------------+---------+--------------------+--------------------+\n",
      "|  1491|00029153d12ae1c9a...|  Organic|00029153d12ae1c9a...|2019-10-18 14:14:...|\n",
      "|  1667|000958fdaefe0dd06...|Instagram|000958fdaefe0dd06...|2019-11-02 14:52:...|\n",
      "|   468|000a3f777828d2cdb...| Facebook|000a3f777828d2cdb...| 2019-11-29 16:33:55|\n",
      "|   780|000c076c390a4c357...|    Naver|000c076c390a4c357...|2019-05-02 15:24:...|\n",
      "|   711|00106ac9184e7d54b...|  Organic|00106ac9184e7d54b...|2019-07-25 18:14:...|\n",
      "|  1653|0012a83c1e6cda9d7...|   Google|0012a83c1e6cda9d7...| 2019-08-07 16:08:41|\n",
      "|    64|001a3678cacf4ea11...|Instagram|001a3678cacf4ea11...|2019-10-04 19:57:...|\n",
      "|  2027|001c4bab90e1fbf3a...| Facebook|001c4bab90e1fbf3a...| 2019-11-20 20:23:10|\n",
      "|   197|001d3439223b7bb23...|    Naver|001d3439223b7bb23...|2019-08-14 15:14:...|\n",
      "|  1513|001f35b87b35111bc...|  Organic|001f35b87b35111bc...|2019-08-05 01:17:...|\n",
      "+------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bk_usc = spark.read.table(\"bk_usc\")\n",
    "df_bk_st = spark.read.table(\"bk_st\")\n",
    "\n",
    "join_expr2 = df_bk_usc.sessionid == df_bk_st.sessionid\n",
    "df_join2 = df_bk_usc.join(df_bk_st, join_expr2, \"inner\")\n",
    "\n",
    "df_join2.show(10)\n",
    "\n",
    "input(\"Waiting ...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
